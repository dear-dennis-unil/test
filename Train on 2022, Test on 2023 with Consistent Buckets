import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt

# Example dataset
# Replace this with your actual dataset
data = pd.read_csv('your_data.csv')

# Define categorical and numerical features
categorical_features = ['cat1', 'cat2', 'cat3', 'cat4', 'cat5']  # Replace with your actual feature names
numerical_features = ['log_premium', 'log_retention']
target = 'log_ultimate_amount'  # Replace with your target column name

# Split data into 2022 (training) and 2023 (testing)
train_data = data[data['year'] == 2022]  # Replace 'year' with your time column
test_data = data[data['year'] == 2023]

# Bucketize 'log_premium' using 2022 data for consistent buckets
train_data['log_premium_bucket'], bins = pd.qcut(train_data['log_premium'], q=10, retbins=True, labels=False)
test_data['log_premium_bucket'] = pd.cut(test_data['log_premium'], bins=bins, labels=False, include_lowest=True)

# One-hot encode categorical features
encoder = OneHotEncoder(drop='first', sparse_output=False)
encoder.fit(train_data[categorical_features])  # Fit on training data

# Prepare results and model storage
results = []
models = {}

# Loop through each bucket
for bucket in range(10):
    print(f"Training model for bucket {bucket}")
    
    # Filter bucket-specific data for training and testing
    train_bucket = train_data[train_data['log_premium_bucket'] == bucket]
    test_bucket = test_data[test_data['log_premium_bucket'] == bucket]

    if train_bucket.empty or test_bucket.empty:  # Skip empty buckets
        print(f"Skipping bucket {bucket} due to no data.")
        continue

    # Process features
    X_train = np.hstack([
        encoder.transform(train_bucket[categorical_features]),
        train_bucket[numerical_features].values
    ])
    y_train = train_bucket[target].values

    X_test = np.hstack([
        encoder.transform(test_bucket[categorical_features]),
        test_bucket[numerical_features].values
    ])
    y_test = test_bucket[target].values

    # Train model
    model = DecisionTreeRegressor(max_depth=5, random_state=42)  # Added depth to reduce overfitting
    model.fit(X_train, y_train)
    models[bucket] = model  # Save the model

    # Evaluate model
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Feature importances
    importances = model.feature_importances_
    feature_names = encoder.get_feature_names_out(categorical_features).tolist() + numerical_features
    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
    importance_df = importance_df.sort_values(by='Importance', ascending=False)

    # Store results
    results.append({
        'Bucket': bucket,
        'MSE': mse,
        'R²': r2,
        'Importances': importance_df
    })

    # Plot predictions
    plt.figure()
    plt.scatter(y_test, y_pred)
    plt.xlabel("Actual")
    plt.ylabel("Predicted")
    plt.title(f"Bucket {bucket}: Actual vs Predicted")
    plt.show()

# Display results for all buckets
for result in results:
    print(f"\nBucket {result['Bucket']}:")
    print(f"MSE: {result['MSE']}")
    print(f"R²: {result['R²']}")
    print("Feature Importances:")
    print(result['Importances'])
